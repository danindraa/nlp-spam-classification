from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Tokenisasi
X = df['clean_text'].values
y = df['label'].values

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X)
X_pad = pad_sequences(tokenizer.texts_to_sequences(X), maxlen=200)

X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42, stratify=y)

# Transformer Model
input_layer = Input(shape=(200,))
embed = Embedding(input_dim=10000, output_dim=128)(input_layer)

attention = MultiHeadAttention(num_heads=4, key_dim=128)(embed, embed)
attention = LayerNormalization()(attention + embed)

ffn = Dense(128, activation='relu')(attention)
ffn = Dropout(0.1)(ffn)
ffn = Dense(64, activation='relu')(ffn)
ffn = GlobalAveragePooling1D()(ffn)

output = Dense(1, activation='sigmoid')(ffn)

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=32)

# Evaluasi
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Akurasi Model Transformer: {accuracy:.4f}")

plt.plot(history.history['accuracy'], label='Training')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Transformer Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
