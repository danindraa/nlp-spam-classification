import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
nltk.download('stopwords')

# 1. Load data
df = pd.read_csv("emails.csv", usecols=['text', 'spam'])
df.columns = ['text', 'label']
df.dropna(inplace=True)
df['label'] = df['label'].astype(int)

# 2. Text cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

df['clean_text'] = df['text'].apply(clean_text)

# 3. Remove stopwords
stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])
df['clean_text'] = df['clean_text'].apply(remove_stopwords)

# 4. Stemming
stemmer = PorterStemmer()
def stem_text(text):
    return ' '.join([stemmer.stem(word) for word in text.split()])
df['clean_text'] = df['clean_text'].apply(stem_text)

# 5. Tokenisasi
df['tokens'] = df['clean_text'].apply(lambda x: x.split())

# 6. BoW
from sklearn.feature_extraction.text import CountVectorizer
vectorizer_bow = CountVectorizer()
X_bow = vectorizer_bow.fit_transform(df['clean_text'])

# 7. TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer_tfidf = TfidfVectorizer()
X_tfidf = vectorizer_tfidf.fit_transform(df['clean_text'])

# 8. Word2Vec
from gensim.models import Word2Vec
w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)
def document_vector(doc):
    doc = [word for word in doc if word in w2v_model.wv]
    return np.mean(w2v_model.wv[doc], axis=0) if doc else np.zeros(100)
X_w2v = np.array([document_vector(doc) for doc in df['tokens']])

# 9. GloVe
glove_embeddings = {}
with open("glove.6B.100d.txt", encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype='float32')
        glove_embeddings[word] = vector
def document_glove_vector(doc):
    doc = [word for word in doc if word in glove_embeddings]
    return np.mean([glove_embeddings[word] for word in doc], axis=0) if doc else np.zeros(100)
X_glove = np.array([document_glove_vector(doc) for doc in df['tokens']])

# 10. FastText
from gensim.models import FastText
ft_model = FastText(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)
def document_vector_fasttext(doc):
    doc = [word for word in doc if word in ft_model.wv]
    return np.mean(ft_model.wv[doc], axis=0) if doc else np.zeros(100)
X_fasttext = np.array([document_vector_fasttext(doc) for doc in df['tokens']])
